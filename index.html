<html>
<head>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

<link rel="shortcut icon" href="images/icon.ico">
<style type="text/css">
	body {
		background-color: #f5f9ff;
	}

	/* Hide both math displays initially, will display based on JS detection */
  .mathjax-mobile, .mathml-non-mobile { display: none; }

  /* Show the MathML content by default on non-mobile devices */
  .show-mathml .mathml-non-mobile { display: block; }
  .show-mathjax .mathjax-mobile { display: block; }

	.content-margin-container {
		display: flex;
		width: 100%; /* Ensure the container is full width */
		justify-content: left; /* Horizontally centers the children in the container */
		align-items: center;  /* Vertically centers the children in the container */
	}
	.main-content-block {
		width: 70%; /* Change this percentage as needed */
    max-width: 1100px; /* Optional: Maximum width */
		background-color: #fff;
		border-left: 1px solid #DDD;
		border-right: 1px solid #DDD;
		padding: 8px 8px 8px 8px;
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
	}
	.margin-left-block {
			font-size: 14px;
			width: 15%; /* Change this percentage as needed */
			max-width: 130px; /* Optional: Maximum width */
			position: relative;
			margin-left: 10px;
			text-align: left;
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
			padding: 5px;
	}
	.margin-right-block {
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
			font-size: 14px;
			width: 25%; /* Change this percentage as needed */
			max-width: 256px; /* Optional: Maximum width */
			position: relative;
			text-align: left;
			padding: 10px;  /* Optional: Adds padding inside the caption */
	}

	img {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	.my-video {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	/* Hide both video displays initially, will display based on JS detection */
  .vid-mobile, .vid-non-mobile { display: none; }

  /* Show the video content by default on non-mobile devices */
  .show-vid-mobile .vid-mobile { display: block; }
  .show-vid-non-mobile .vid-non-mobile { display: block; }

	a:link,a:visited
	{
		color: #0e7862; /*#1367a7;*/
		text-decoration: none;
	}
	a:hover {
		color: #24b597; /*#208799;*/
	}

	h1 {
		font-size: 18px;
		margin-top: 4px;
		margin-bottom: 10px;
	}

	table.header {
    font-weight: 300;
    font-size: 17px;
    flex-grow: 1;
		width: 70%;
    max-width: calc(100% - 290px); /* Adjust according to the width of .paper-code-tab */
	}
	table td, table td * {
	    vertical-align: middle;
	    position: relative;
	}
	table.paper-code-tab {
	    flex-shrink: 0;
	    margin-left: 8px;
	    margin-top: 8px;
	    padding: 0px 0px 0px 8px;
	    width: 290px;
	    height: 150px;
	}

	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	hr {
    height: 1px; /* Sets the height of the line to 1 pixel */
    border: none; /* Removes the default border */
    background-color: #DDD; /* Sets the line color to black */
  }

	div.hypothesis {
		width: 80%;
		background-color: #EEE;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
		font-family: Courier;
		font-size: 18px;
		text-align: center;
		margin: auto;
		padding: 16px 16px 16px 16px;
	}

	div.citation {
    font-size: 0.8em;
    background-color:#fff;
    padding: 10px;
		height: 200px;
  }

	.fade-in-inline {
		position: absolute;
		text-align: center;
		margin: auto;
		-webkit-mask-image: linear-gradient(to right,
																			transparent 0%,
																			transparent 40%,
																			black 50%,
																			black 90%,
																			transparent 100%);
		mask-image: linear-gradient(to right,
																transparent 0%,
																transparent 40%,
																black 50%,
																black 90%,
																transparent 100%);
		-webkit-mask-size: 8000% 100%;
		mask-size: 8000% 100%;
		animation-name: sweepMask;
		animation-duration: 4s;
		animation-iteration-count: infinite;
		animation-timing-function: linear;
		animation-delay: -1s;
	}

	.fade-in2-inline {
			animation-delay: 1s;
	}

	.inline-div {
			position: relative;
	    display: inline-block; /* Makes both the div and paragraph inline-block elements */
	    vertical-align: top; /* Aligns them at the top, you can adjust this to middle, bottom, etc., based on your needs */
	    width: 50px; /* Optional: Adds space between the div and the paragraph */
	}

</style>

	  <title>6.7960 Final Project </title>
      <meta property="og:title" content="The Platonic Representation Hypothesis" />
			<meta charset="UTF-8">
  </head>

  <body>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<table class="header" align=left>
								<tr>
									<td colspan=4>
										<span style="font-size: 32px; font-family: 'Courier New', Courier, monospace; /* Adds fallbacks */"> 6.7960 Final Project</span>
									</td>
								</tr>
								<tr>
										<td align=left>
												<span style="font-size:17px"><a href="your_website">Juan Luera</a></span>
										</td>
										<td align=left>
												<span style="font-size:17px"><a href="your_partner's_website">Gyalpo Dongo</a></span>
										</td>
								<tr>
									<td colspan=4 align=left><span style="font-size:18px">Final project for 6.7960, MIT</span></td>
								</tr>
						</table>
					</div>
					<div class="margin-right-block">
					</div>
		</div>

		<div class="content-margin-container" id="intro">
				<div class="margin-left-block">
          <!-- table of contents here -->
          <div style="position:fixed; max-width:inherit; top:max(20%,120px)">
              <b style="font-size:16px">Outline</b><br><br>
              <a href="#intro">Introduction</a><br><br>
              <a href="#background_and_related_work">Background and Related Work</a><br><br>
              <a href="#methods_and_experiment">Methods and Experiment</a><br><br>
			  <a href="#result_and_analysis">Result and Analysis</a><br><br>
			  <a href="#discussion_and_conclusion">Discussion and Conclusion</a><br><br>
          </div>
				</div>
		    <div class="main-content-block">
            <!--You can embed an image like this:-->
            <img src="./images/your_image_here.png" width=512px/>
		    </div>
		    <div class="margin-right-block">
						Caption for the image.
		    </div>
		</div>

    <div class="content-margin-container" id="intro">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Introduction</h1>
						<p>
							RLHF is a method used to fine-tune LLMs (Large Language Models) to better align with human values. As the capabilities of LLMs have increased so has their potential to cause harm, ever increasing the importance of value alignment.  As discussed in (Zheng et al.) "Despite the capacities, since LLMs are trained to capture the data characteristics of pre-training corpora (including both high-quality and low-quality data), these models are likely to express unintended behaviors such as making up facts, generating biased or toxic text, or even harmful content for humans”. Alignment Algorithms such as RLHF aim to address these issues by training models that are helpful, honest, and harmless. 

    <p><strong>Problem Statement:</strong> Most recent work in LLM alignment has primarily relied on PPO, or in alternatives such as KTO, and DPO. Techniques such as KTO and DPO aim to simplify RLHF by avoiding a reward model, but fail to capitalize on the potential benefits that online learning that algorithms such as PPO provide.  Drawing inspiration from computational game theory, specifically the superior convergence properties of Optimistic Multiplicative Weights Update (OMWU), we propose incorporating optimism into PPO.
      
    </p>
    <p><strong>Thesis Statement:</strong>   This novel approach could potentially combine the theoretical benefits of optimistic algorithms such as better genralization with the practical success of PPO in LLM alignment.</div>
		    <div class="margin-right-block">
						Margin note that clarifies some detail #main-content-block for intro section.
		    </div>
		</div>

		<div class="content-margin-container" id="background_and_related_work">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
					<h1>Background and Related Work</h1> 
					<p><strong>What is RLHF</strong> Reinforcement Learning from Human Feedback is a machine learning technique that combines hu-
						man input with reinforcement learning in order to align LLMs with human values. This technique
						is particularly useful when you struggle to define success. In certain tasks, such as ethical decisions
						or creating chat bots that talk like a human its often hard to have a mathematical threshold for how
						good a model is. RLHF is able to perform this task by fine-tuning models so that they act/respond
						as a we would like them to. RLHF can be divided into three stages:

						<ul>
							<li>
								<strong>Pretraining:</strong> The model that you want to fine-tune is pre-trained using traditional supervised learning methods (supervised fine-tuning). This gives the model a baseline alignment and a foundational understanding of language.
							</li>
							<li>
								<strong>Reward Model Training:</strong> Human feedback is collected and used to train a "reward model." In other words, humans evaluate the outputs of a model based on a certain metric. This evaluation data is collected and used to train the reward model. This model is used in the RLHF loop to assign rewards/values to the outputs of the pre-trained model.
							</li>
							<li>
								<strong>Reinforcement Learning:</strong> Using the reward model outputs as guidance, the model undergoes reinforcement learning. Fundamentally, what happens is that a reinforcement learning algorithm called Proximal Policy Optimization outputs a loss value that is used to update the weights of the pre-trained model. This is repeated for every prompt until you’re left with a model that is closely aligned to human values.
							</li>
						</ul>
					
					</p>

					<p><strong>Why is Reinforcement Learning from Human Feedback (RLHF) Important?</strong> While, there exist other ways to fine-tune models RLHF has proven to me one of the most robust and
						scalable. Compared to traditional reinforcement learning that relies on predefined rewards (which
						often aren’t complex enough to represent human values) RLHF is able to capture the complexity of
						human morality better due to to feedback it receives from the reward model. Furthermore, it doesn’t
						require human annotators at every step which makes it scalable. This flexibility and scalability have
						made RLHF ideal for:
						<ul>
							<li>Natural Language Processing: Improving chatbot responses to be more coherent, and polite.
							</li>
							<li>
								Ethical Decision-Making: Training autonomous systems like self-driving cars to reflect hu-
								man ethical judgments in complex scenarios.
							</li>
							<li>
								Creative Tasks: Enhancing generative models for art, writing, or humor where subjective human preferences are important. For instance AI systemsm like Chat GPT use RLHF to ensure their responses
								align with the users ethical considerations and expectations.</li>
						</ul>
						<p>
					<p><strong>
						What are the Limitations of RLHF
					</strong> 
						While RLHF is one of the best ways to perform alignment that we currently know of, it’s not per-
						fect. One limitation is its ability to respond to bias. Human feedback can be biased or inconsistent
						leading to suboptimal reward model that might not align with human values. Furthermore, PPO is
						a conservative algorithm that may not always discover the optimal alignments. One way to address
						these limitations is to use a different different reinforcement learning algorithm such as optimistic
						PPO.</p>
					<p><strong>Overview of PPO in RLHF:</strong> Explain PPO’s role in policy optimization and its clipped objective function for stable training. Discuss its success in aligning LLMs but note its limitations in convergence speed and stability.</p>
					<p><strong>Simplified Preference Learning Methods:</strong> Summarize DPO (Rafailov et al., 2023) and KTO (Ethayarajh et al., 2023), emphasizing their simplicity but lack of online learning capabilities.</p>
					<p><strong>Optimistic Algorithms:</strong> Introduce Optimistic Multiplicative Weights Update (OMWU) from computational game theory, highlighting its superior convergence properties.</p>
					<p><strong>Research Gap:</strong> Emphasize that no prior work has incorporated optimism into PPO for preference learning, making this a novel contribution.</p><br><br>

         
          

         
		    </div>

		    <div class="margin-right-block" style="transform: translate(0%, -100%);"> <!-- you can move the margin notes up and down with translate -->
          Interestingly, Plato also asked if X does Y, in <a href="#ref_1">[1]</a>.
		    </div>
		</div>

		

		<div class="content-margin-container" id="methods_and_experiment">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Methods and Experimet</h1>
						<h2>Proximal Policy Optimization (PPO)</h2>
    <p>Proximal Policy Optimization (PPO) is a reinforcement learning algorithm designed to improve
		training stability and efficiency.PPO Loss function shown below:</p>
    <math xmlns="http://www.w3.org/1998/Math/MathML">
		<mrow>
		  <mi>L</mi>
		  <mo>(</mo>
		  <mi>s</mi>
		  <mo>,</mo>
		  <mi>a</mi>
		  <mo>,</mo>
		  <mi>&theta;</mi>
		  <msub><mi>k</mi></msub>
			
		  
		  <mo>,</mo>
		  <mi>&theta;</mi>
		  <mo>)</mo>
		  <mo>=</mo>
		  <mi>min</mi>
		  <mo>(</mo>
		  <mrow>
			<mfrac>
			  <mrow>
				<mi>&pi;</mi>
				<mi>&theta;</mi>
				<mo>(</mo>
				<mi>a</mi>
				<mo>|</mo>
				<mi>s</mi>
				<mo>)</mo>
			  </mrow>
			  <mrow>
				<mi>&pi;</mi>
				<mi>&theta;</mi>
				
				  <mi>k</mi>
				
				<mo>(</mo>
				<mi>a</mi>
				<mo>|</mo>
				<mi>s</mi>
				<mo>)</mo>
			  </mrow>
			</mfrac>
			<mi>A</mi>
			<msup>
			  <mi>&pi;</mi>
			  <mi>&theta;</mi>
			  <mi>k</mi>
			</msup>
			<mo>(</mo>
			<mi>s</mi>
			<mo>,</mo>
			<mi>a</mi>
			<mo>)</mo>
		  </mrow>
		  <mo>,</mo>
		  <mrow>
			<mi>clip</mi>
			<mo>(</mo>
			<mfrac>
			  <mrow>
				<mi>&pi;</mi>
				<mi>&theta;</mi>
				<mo>(</mo>
				<mi>a</mi>
				<mo>|</mo>
				<mi>s</mi>
				<mo>)</mo>
			  </mrow>
			  <mrow>
				<mi>&pi;</mi>
				<mi>&theta;</mi>
				
				  <mi>k</mi>
				
				<mo>(</mo>
				<mi>a</mi>
				<mo>|</mo>
				<mi>s</mi>
				<mo>)</mo>
			  </mrow>
			</mfrac>
			<mo>,</mo>
			<mrow>
			  <mn>1</mn>
			  <mo>-</mo>
			  <mi>&epsilon;</mi>
			</mrow>
			<mo>,</mo>
			<mrow>
			  <mn>1</mn>
			  <mo>+</mo>
			  <mi>&epsilon;</mi>
			</mrow>
			<mo>)</mo>
			<mi>A</mi>
			<msup>
			  <mi>&pi;</mi>
			  <mi>&theta;</mi>
			  <mi>k</mi>
			</msup>
			<mo>(</mo>
			<mi>s</mi>
			<mo>,</mo>
			<mi>a</mi>
			<mo>)</mo>
		  </mrow>
		  <mo>)</mo>
		</mrow>
	  </math>
	  
	  <p><strong>Explanation of Terms</strong> </p>
	  <ol>
		  <li>
			  <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
				  <mfrac>
					  <msub><mi>π</mi><mi>θ</mi></msub>
					  <msub><mi>π</mi><msub><mi>θ</mi><mi>k</mi></msub></msub>
				  </mfrac>
				  <mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo>:
			  </math> 
			  The ratio of the current policy’s probability of taking action 
			  <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>a</mi></math> 
			  in state 
			  <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>s</mi></math> 
			  to the old policy’s probability. This measures how much the new policy deviates from the old one.
		  </li>
		  <mo>(</mo>
		  <mi>a</mi>
		  <mo>|</mo>
		  <mi>s</mi>
		  <mo>)</mo>  
		  <li>
			  <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
				  <msub><mi>A</mi><msub><mi>π</mi><msub><mi>θ</mi><mi>k</mi></msub></msub></msub>
				  <mo>(</mo><mi>a</mi> <mo>|</mo><mi>a</mi><mo>)</mo>:
			  </math> 
			  The advantage function under the old policy, which indicates how much better an action 
			  <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>a</mi></math> 
			  is compared to the average action in state 
			  <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>s</mi></math>. It is defined as:
			  <br />
			  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
				  <msub><mi>A</mi><msub><mi>π</mi><msub><mi>θ</mi><mi>k</mi></msub></msub></msub>
				  <mo>(</mo><mi>s</mi> <mo>,</mo><mi>a</mi><mo>) = </mo>
				  <msub><mi>Q</mi><msub><mi>π</mi><msub><mi>θ</mi><mi>k</mi></msub></msub></msub>
				  <mo>(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>) - </mo>
				  <msub><mi>V</mi><msub><mi>π</mi><msub><mi>θ</mi><mi>k</mi></msub></msub></msub>
				  <mo>(</mo><mi>s</mi><mo>)</mo>.
			  </math>
		  </li>
  
		  <li>
			  <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
				ϵ
			  </math>ϵ: The clipping parameter that limits how much the ratio 
			  <math xmlns="http://www.w3.org/1998/Math/MathML">
				  <mfrac>
					  <msub><mi>π</mi><mi>θ</mi></msub>
					  <msub><mi>π</mi><msub><mi>θ</mi><mn>k</mn></msub></msub>
				  </mfrac>
			  </math>
			  can deviate from 1. This ensures that policy updates remain stable and do not change too much.
		  </li>
  
		  
	  </ol>
  
	  <p>
		  Due to this construction that rewards alignment and punishes large deviations, PPO has become a widely used robust algorithm for fine-tuning.

		 <p><Strong>From PPO to PPO</Strong></p>
    <p>PPO and OPPO are quite similar in construction; in fact, they’re nearly identical. This is because we like the benefits that PPO offers and want to use them. The difference between the two is the advantage function.</p>
    
    <p><strong>General Advantage Function</strong> </p>
    <p>Recall that in PPO our advantage function was the following:</p>
    <p>
        <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
			<msup>
				<mi>A</mi>
				<mi>π</mi>
			</msup>
			<mo>(</mo>
			<mi>s</mi>
			<mo>,</mo>
			<mi>a</mi>
			<mo>) = </mo>
			<msup>
				<mi>Q</mi>
				<mi>π</mi>
			</msup>
			<mo>(</mo>
			<mi>s</mi>
			<mo>,</mo>
			<mi>a</mi>
			<mo>) - </mo>
			<msup>
				<mi>V</mi>
				<mi>π</mi>
			</msup>
			<mo>(</mo>
			<mi>s</mi>
			<mo>)</mo>.
		</math>
    </p>
    <p>where:</p>
    <ul>
        <li>
            <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
                <msup><mi>Q</mi><mi>π</mi></msup><mo>(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>)</mo>:
            </math> 
            The expected return for taking action 
            <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>a</mi></math> 
            in state 
            <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>s</mi></math>.
        </li>

        <li>
            <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
                <msup><mi>V</mi><mi>π</mi></msup><mo>(</mo><mi>s</mi><mo>)</mo>:
            </math> 
            The expected return for being in state 
            <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>s</mi></math>.
        </li>
    </ul>

    <h2>3.3 Experimental Setup</h2>
    <ul>
      <li><strong>Evaluation metrics:</strong> Convergence rate, stability, preference alignment quality, computational efficiency.</li>
      <li><strong>Dataset and Tasks:</strong> Specify datasets used for alignment experiments (e.g., preference-labeled text data).</li>
    </ul>
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

		<div class="content-margin-container" id="result_and_analysis">
			<div class="margin-left-block"></div>
			<div class="main-content-block">
			  <h1>4. Results and Analysis</h1>
		  
			  <h2>4.1 Quantitative Results</h2>
			  <p>- Present comparative results across methods (Optimistic PPO, standard PPO, DPO, KTO): convergence rates over training epochs, stability metrics across multiple runs, preference alignment scores.</p>
		  
			  <h2>4.2 Qualitative Insights</h2>
			  <p>- Analyze why Optimistic PPO achieves faster convergence or greater stability compared to baselines.</p>
		  
			  <h2>4.3 Computational Efficiency</h2>
			  <p>- Compare resource usage across methods to evaluate practicality.</p>
		  
			</div>
			<div class="margin-right-block"></div>
		</div>

		<div class="content-margin-container" id="discussion_and_conclusion">
			<div class="margin-left-block"></div>
			<div class="main-content-block">
			  <h1>5. Discussion and Conclusion</h1>
		  
			  <h2>5.1 Key Findings</h2>
			  <p>- Summarize the main outcomes: faster convergence and improved stability with Optimistic PPO; comparable or superior alignment quality relative to DPO/KTO.</p>
		  
			  <h2>5.2 Implications</h2>
			  <p>- Discuss how Optimistic PPO could influence future RLHF methods and LLM alignment research.</p>
		  
			  <h2>5.3 Limitations</h2>
			  <ul>
				<li>- Added complexity in implementation.</li>
				<li>- Scalability concerns for larger models or datasets.</li>
			  </ul>
		  
			  <h2>5.4 Future Directions</h2>
			  - Suggest areas for further exploration: extending Optimistic PPO to other RL tasks beyond preference learning.
			</div>
			<div class="margin-right-block"></div>
		  </div>
		
		<div class="content-margin-container" id="citations">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<div class='citation' id="references" style="height:auto"><br>
							<span style="font-size:16px">References:</span><br><br>
							<a id="ref_1"></a>[1] Rafailov et al. (2023). "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"<br><br>
        					<a id="ref_2"></a>[2] Ethayarajh et al. (2023). "KTO: Model Alignment as Prospect Theoretic Optimization"<br><br>
        					<a id="ref_3"></a>[3] Schulman et al. (2017). "Proximal Policy Optimization Algorithms"<br><br>
        					<a id="ref_4"></a>[4] Zheng et al. (2023). "Delve into PPO: Implementation Matters for Stable RLHF"<br><br>
						</div>
		    </div>
		    <div class="margin-right-block">
            <!-- margin notes for reference block here -->
		    </div>
		</div>

	</body>

</html>
